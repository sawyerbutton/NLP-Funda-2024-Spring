{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXVFx1TUe9Rc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from copy import deepcopy\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from datasets import load_dataset, set_caching_enabled\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    # Preprocessing / Common\n",
        "    AutoTokenizer, AutoFeatureExtractor,\n",
        "    # Text & Image Models (Now, image transformers like ViTModel, DeiTModel, BEiT can also be loaded using AutoModel)\n",
        "    AutoModel,\n",
        "    # Training / Evaluation\n",
        "    TrainingArguments, Trainer,\n",
        "    # Misc\n",
        "    logging\n",
        ")\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SET CACHE FOR HUGGINGFACE TRANSFORMERS + DATASETS\n",
        "os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n",
        "# SET ONLY 1 GPU DEVICE\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "set_caching_enabled(True)\n",
        "logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "0WpGBaJffO9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "#     print('Memory Usage:')\n",
        "#     print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "#     print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
      ],
      "metadata": {
        "id": "A2es_I4_fQ_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files={\n",
        "        \"train\": os.path.join(\"..\", \"dataset\", \"data_train.csv\"),\n",
        "        \"test\": os.path.join(\"..\", \"dataset\", \"data_eval.csv\")\n",
        "    }\n",
        ")\n",
        "\n",
        "with open(os.path.join(\"..\", \"dataset\", \"answer_space.txt\")) as f:\n",
        "    answer_space = f.read().splitlines()\n",
        "\n",
        "dataset = dataset.map(\n",
        "    lambda examples: {\n",
        "        'label': [\n",
        "            answer_space.index(ans.replace(\" \", \"\").split(\",\")[0]) # Select the 1st answer if multiple answers are provided\n",
        "            for ans in examples['answer']\n",
        "        ]\n",
        "    },\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "2q5Rc9wMfTYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "def showExample(train=True, id=None):\n",
        "    if train:\n",
        "        data = dataset[\"train\"]\n",
        "    else:\n",
        "        data = dataset[\"test\"]\n",
        "    if id == None:\n",
        "        id = np.random.randint(len(data))\n",
        "    image = Image.open(os.path.join(\"..\", \"dataset\", \"images\", data[id][\"image_id\"] + \".png\"))\n",
        "    display(image)\n",
        "\n",
        "    print(\"Question:\\t\", data[id][\"question\"])\n",
        "    print(\"Answer:\\t\\t\", data[id][\"answer\"], \"(Label: {0})\".format(data[id][\"label\"]))"
      ],
      "metadata": {
        "id": "RvMZjudffY-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "showExample()"
      ],
      "metadata": {
        "id": "IbbXsvzlfch7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class MultimodalCollator:\n",
        "    tokenizer: AutoTokenizer\n",
        "    preprocessor: AutoFeatureExtractor\n",
        "\n",
        "    def tokenize_text(self, texts: List[str]):\n",
        "        encoded_text = self.tokenizer(\n",
        "            text=texts,\n",
        "            padding='longest',\n",
        "            max_length=24,\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoded_text['input_ids'].squeeze(),\n",
        "            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n",
        "            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n",
        "        }\n",
        "\n",
        "    def preprocess_images(self, images: List[str]):\n",
        "        processed_images = self.preprocessor(\n",
        "            images=[Image.open(os.path.join(\"..\", \"dataset\", \"images\", image_id + \".png\")).convert('RGB') for image_id in images],\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n",
        "        }\n",
        "\n",
        "    def __call__(self, raw_batch_dict):\n",
        "        return {\n",
        "            **self.tokenize_text(\n",
        "                raw_batch_dict['question']\n",
        "                if isinstance(raw_batch_dict, dict) else\n",
        "                [i['question'] for i in raw_batch_dict]\n",
        "            ),\n",
        "            **self.preprocess_images(\n",
        "                raw_batch_dict['image_id']\n",
        "                if isinstance(raw_batch_dict, dict) else\n",
        "                [i['image_id'] for i in raw_batch_dict]\n",
        "            ),\n",
        "            'labels': torch.tensor(\n",
        "                raw_batch_dict['label']\n",
        "                if isinstance(raw_batch_dict, dict) else\n",
        "                [i['label'] for i in raw_batch_dict],\n",
        "                dtype=torch.int64\n",
        "            ),\n",
        "        }"
      ],
      "metadata": {
        "id": "arT0QhVCfeXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalVQAModel(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_labels: int = len(answer_space),\n",
        "            intermediate_dim: int = 512,\n",
        "            pretrained_text_name: str = 'bert-base-uncased',\n",
        "            pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'):\n",
        "\n",
        "        super(MultimodalVQAModel, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.pretrained_text_name = pretrained_text_name\n",
        "        self.pretrained_image_name = pretrained_image_name\n",
        "\n",
        "        self.text_encoder = AutoModel.from_pretrained(\n",
        "            self.pretrained_text_name,\n",
        "        )\n",
        "        self.image_encoder = AutoModel.from_pretrained(\n",
        "            self.pretrained_image_name,\n",
        "        )\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids: torch.LongTensor,\n",
        "            pixel_values: torch.FloatTensor,\n",
        "            attention_mask: Optional[torch.LongTensor] = None,\n",
        "            token_type_ids: Optional[torch.LongTensor] = None,\n",
        "            labels: Optional[torch.LongTensor] = None):\n",
        "\n",
        "        encoded_text = self.text_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        encoded_image = self.image_encoder(\n",
        "            pixel_values=pixel_values,\n",
        "            return_dict=True,\n",
        "        )\n",
        "                fused_output = self.fusion(\n",
        "            torch.cat(\n",
        "                [\n",
        "                    encoded_text['pooler_output'],\n",
        "                    encoded_image['pooler_output'],\n",
        "                ],\n",
        "                dim=1\n",
        "            )\n",
        "        )\n",
        "        logits = self.classifier(fused_output)\n",
        "\n",
        "        out = {\n",
        "            \"logits\": logits\n",
        "        }\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(logits, labels)\n",
        "            out[\"loss\"] = loss\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "l_IgMBN2fi2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createMultimodalVQACollatorAndModel(text='bert-base-uncased', image='google/vit-base-patch16-224-in21k'):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(text)\n",
        "    preprocessor = AutoFeatureExtractor.from_pretrained(image)\n",
        "\n",
        "    multi_collator = MultimodalCollator(\n",
        "        tokenizer=tokenizer,\n",
        "        preprocessor=preprocessor,\n",
        "    )\n",
        "\n",
        "\n",
        "    multi_model = MultimodalVQAModel(pretrained_text_name=text, pretrained_image_name=image).to(device)\n",
        "    return multi_collator, multi_model"
      ],
      "metadata": {
        "id": "4nQFCbszfo2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wup_measure(a,b,similarity_threshold=0.925):\n",
        "    \"\"\"\n",
        "    Returns Wu-Palmer similarity score.\n",
        "    More specifically, it computes:\n",
        "        max_{x \\in interp(a)} max_{y \\in interp(b)} wup(x,y)\n",
        "        where interp is a 'interpretation field'\n",
        "    \"\"\"\n",
        "    def get_semantic_field(a):\n",
        "        weight = 1.0\n",
        "        semantic_field = wordnet.synsets(a,pos=wordnet.NOUN)\n",
        "        return (semantic_field,weight)\n",
        "\n",
        "\n",
        "    def get_stem_word(a):\n",
        "        \"\"\"\n",
        "        Sometimes answer has form word\\d+:wordid.\n",
        "        If so we return word and downweight\n",
        "        \"\"\"\n",
        "        weight = 1.0\n",
        "        return (a,weight)\n",
        "\n",
        "\n",
        "    global_weight=1.0\n",
        "\n",
        "    (a,global_weight_a)=get_stem_word(a)\n",
        "    (b,global_weight_b)=get_stem_word(b)\n",
        "    global_weight = min(global_weight_a,global_weight_b)\n",
        "\n",
        "    if a==b:\n",
        "        # they are the same\n",
        "        return 1.0*global_weight\n",
        "\n",
        "    if a==[] or b==[]:\n",
        "        return 0\n",
        "\n",
        "\n",
        "    interp_a,weight_a = get_semantic_field(a)\n",
        "    interp_b,weight_b = get_semantic_field(b)\n",
        "\n",
        "    if interp_a == [] or interp_b == []:\n",
        "        return 0\n",
        "\n",
        "    # we take the most optimistic interpretation\n",
        "    global_max=0.0\n",
        "    for x in interp_a:\n",
        "        for y in interp_b:\n",
        "            local_score=x.wup_similarity(y)\n",
        "            if local_score > global_max:\n",
        "                global_max=local_score\n",
        "\n",
        "    # we need to use the semantic fields and therefore we downweight\n",
        "    # unless the score is high which indicates both are synonyms\n",
        "    if global_max < similarity_threshold:\n",
        "        interp_weight = 0.1\n",
        "    else:\n",
        "        interp_weight = 1.0\n",
        "\n",
        "    final_score=global_max*weight_a*weight_b*interp_weight*global_weight\n",
        "    return final_score"
      ],
      "metadata": {
        "id": "s85RDZTZfqpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_wup_measure(labels, preds):\n",
        "    wup_scores = [wup_measure(answer_space[label], answer_space[pred]) for label, pred in zip(labels, preds)]\n",
        "    return np.mean(wup_scores)"
      ],
      "metadata": {
        "id": "PLq55HN2fv6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.random.randint(len(answer_space), size=5)\n",
        "preds = np.random.randint(len(answer_space), size=5)\n",
        "\n",
        "def showAnswers(ids):\n",
        "    print([answer_space[id] for id in ids])\n",
        "\n",
        "showAnswers(labels)\n",
        "showAnswers(preds)\n",
        "\n",
        "print(\"Predictions vs Labels: \", batch_wup_measure(labels, preds))\n",
        "print(\"Labels vs Labels: \", batch_wup_measure(labels, labels))"
      ],
      "metadata": {
        "id": "qVhNj3mFfyP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
        "    logits, labels = eval_tuple\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    return {\n",
        "        \"wups\": batch_wup_measure(labels, preds),\n",
        "        \"acc\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average='macro')\n",
        "    }"
      ],
      "metadata": {
        "id": "TZ-AmQhLf0CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"checkpoint\",\n",
        "    seed=12345,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,             # Save only the last 3 checkpoints at any given time while training\n",
        "    metric_for_best_model='wups',\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    remove_unused_columns=False,\n",
        "    num_train_epochs=5,\n",
        "    fp16=True,\n",
        "    # warmup_ratio=0.01,\n",
        "    # learning_rate=5e-4,\n",
        "    # weight_decay=1e-4,\n",
        "    # gradient_accumulation_steps=2,\n",
        "    dataloader_num_workers=8,\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "VFzqARref1o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createAndTrainModel(dataset, args, text_model='bert-base-uncased', image_model='google/vit-base-patch16-224-in21k', multimodal_model='bert_vit'):\n",
        "    collator, model = createMultimodalVQACollatorAndModel(text_model, image_model)\n",
        "\n",
        "    multi_args = deepcopy(args)\n",
        "    multi_args.output_dir = os.path.join(\"..\", \"checkpoint\", multimodal_model)\n",
        "    multi_trainer = Trainer(\n",
        "        model,\n",
        "        multi_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['test'],\n",
        "        data_collator=collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    train_multi_metrics = multi_trainer.train()\n",
        "    eval_multi_metrics = multi_trainer.evaluate()\n",
        "\n",
        "    return collator, model, train_multi_metrics, eval_multi_metrics"
      ],
      "metadata": {
        "id": "DmZE9CWaf3sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collator, model, train_multi_metrics, eval_multi_metrics = createAndTrainModel(dataset, args)"
      ],
      "metadata": {
        "id": "F0EgICdAf5oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_multi_metrics"
      ],
      "metadata": {
        "id": "Q1rJI1cBf7yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultimodalVQAModel()\n",
        "\n",
        "# We use the checkpoint giving best results\n",
        "model.load_state_dict(torch.load(os.path.join(\"..\", \"checkpoint\", \"bert_vit\", \"checkpoint-1500\", \"pytorch_model.bin\")))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "SZvr0wC8f9ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = collator(dataset[\"test\"][2000:2005])\n",
        "\n",
        "input_ids = sample[\"input_ids\"].to(device)\n",
        "token_type_ids = sample[\"token_type_ids\"].to(device)\n",
        "attention_mask = sample[\"attention_mask\"].to(device)\n",
        "pixel_values = sample[\"pixel_values\"].to(device)\n",
        "labels = sample[\"labels\"].to(device)"
      ],
      "metadata": {
        "id": "eJT7NPq7f_xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "output = model(input_ids, pixel_values, attention_mask, token_type_ids, labels)"
      ],
      "metadata": {
        "id": "UYRJqDi8gDmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = output[\"logits\"].argmax(axis=-1).cpu().numpy()\n",
        "preds"
      ],
      "metadata": {
        "id": "RRqC6rTpgF9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2000, 2005):\n",
        "    print(\"*********************************************************\")\n",
        "    showExample(train=False, id=i)\n",
        "    print(\"Predicted Answer:\\t\", answer_space[preds[i-2000]])\n",
        "    print(\"*********************************************************\")"
      ],
      "metadata": {
        "id": "VSsOz6AygHmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def countTrainableParameters(model):\n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(\"No. of trainable parameters:\\t{0:,}\".format(num_params))"
      ],
      "metadata": {
        "id": "FxQN0z6mgKEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countTrainableParameters(model) # For BERT-ViT model"
      ],
      "metadata": {
        "id": "msSSLjWRgMYm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}