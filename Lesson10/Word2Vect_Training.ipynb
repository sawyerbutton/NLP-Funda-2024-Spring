{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhUHZ-dcv050"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORRobvX_xDHh"
      },
      "outputs": [],
      "source": [
        "#basic LIB\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "import sys\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "#dataset\n",
        "from keras.datasets import imdb\n",
        "#Visulization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import HTML,display\n",
        "import random\n",
        "#Preprosessing\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import Word2Vec,Phrases\n",
        "from gensim.models import KeyedVectors\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "#Model Building\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Conv1D, MaxPool1D,GlobalMaxPooling1D, Dense, Dropout , LSTM,Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import legacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E4h3KRkv6xv"
      },
      "outputs": [],
      "source": [
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJdjuZl1v9Ti"
      },
      "outputs": [],
      "source": [
        "# Limit GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d7FLtgfxYwR"
      },
      "outputs": [],
      "source": [
        "print(tf.test.gpu_device_name())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exb70Mxsxb6U"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whtgxKKXxdO6"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')  # download the corpus\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_zFURxCxfNc"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWsvhWaSxhY9"
      },
      "outputs": [],
      "source": [
        "num_words = 10000 # Change this to adjust the vocabulary size\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "# Decode the reviews back to English words\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_jYQMWYxm9E"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Decode all reviews in the training data\n",
        "reviews = []\n",
        "for review in train_data:\n",
        "    decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in review]) # <pad> <Sos> <unk>\n",
        "    reviews.append(decoded_review)\n",
        "\n",
        "# Decode all reviews in the test data\n",
        "for review in test_data:\n",
        "    decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in review])\n",
        "    reviews.append(decoded_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSYbtNxUxq5x"
      },
      "outputs": [],
      "source": [
        "# combine all labels\n",
        "Labels = []\n",
        "for label in train_labels:\n",
        "    Labels.append(label)\n",
        "test_reviews = []\n",
        "for label in test_labels:\n",
        "    Labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miawQIlnxsB9"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'review': reviews, 'sentiment': Labels})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btts2dODxtWY"
      },
      "outputs": [],
      "source": [
        "df['sentiment']=df['sentiment'].map({ 1 : 'positive',  0 : 'negative'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpB5nx1YxwMp"
      },
      "outputs": [],
      "source": [
        "df.info()\n",
        "#our data consist od 50K records with 2 columns 1 feature and 1 Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp_ABkj6x0nP"
      },
      "outputs": [],
      "source": [
        "df.groupby(['sentiment']).describe()\n",
        "#25K Positive and 25 K negative  thus our data is balanced and with 24698 , 24884 unique words for each\n",
        "#and  3 , 5  Frequency\n",
        "#let's combine them and see what's total uniqe value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SP3pcNWx1tH"
      },
      "outputs": [],
      "source": [
        "df['review'].nunique()\n",
        "# 49582 unique word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMc7ckqGx3As"
      },
      "outputs": [],
      "source": [
        "#let's check if there is duplicate values\n",
        "duplicates = df[df.duplicated()]\n",
        "print(f'Number of duplicate rows: {len(duplicates)}')\n",
        "#yes there is duplicated values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDIb1wVTx4Mg"
      },
      "outputs": [],
      "source": [
        "#let's check for null\n",
        "df.isnull().sum()\n",
        "#there is no null values at any column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgNUuEaax5TW"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=df['sentiment']) #ploting distribution for easier understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3veZnmHx6qv"
      },
      "outputs": [],
      "source": [
        "#let's see our dataframe\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hRLejCKx8jH"
      },
      "outputs": [],
      "source": [
        "#let's see how data is looklike\n",
        "random_index=random.randint(0,df.shape[0]-3)\n",
        "for row in df[['review','sentiment']][random_index:random_index+3].itertuples():\n",
        "    _,text,label=row\n",
        "    class_name=\"Positive\"\n",
        "    if label==0:\n",
        "        class_name=\"Negative\"\n",
        "    display(HTML(f\"<h5><b style='color:red'>Text: </b>{text}</h5>\"))\n",
        "    display(HTML(f\"<h5><b style='color:red'>Target: </b>{class_name}<br><hr></h5>\"))\n",
        "#data contain so much garbage needs to be cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7rUilysx9xA"
      },
      "outputs": [],
      "source": [
        "positivedata = df[df['sentiment']== 'positive']\n",
        "positivedata =positivedata['review']\n",
        "negdata = df[df['sentiment']== 'negative']\n",
        "negdata= negdata['review']\n",
        "\n",
        "def wordcloud_draw(data, color, s):\n",
        "    words = ' '.join(data)\n",
        "    cleaned_word = \" \".join([word for word in words.split() if (word not in ['movie', 'film'])])\n",
        "    if not cleaned_word:\n",
        "        cleaned_word = 'no_data'  # Set a default value if there are no words\n",
        "    word_counts = Counter(cleaned_word.split())\n",
        "    if not word_counts:\n",
        "        word_counts = {'no_data': 1}  # Set a default value if there are no words\n",
        "    wordcloud = WordCloud(background_color=color, width=2500, height=2000).generate_from_frequencies(word_counts)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.title(s)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.figure(figsize=[20,10])\n",
        "plt.subplot(1,2,1)\n",
        "wordcloud_draw(positivedata,'white','Most-common Positive words')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "wordcloud_draw(negdata, 'white','Most-common Negative words')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHwplxfVyAaJ"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(inplace=True)\n",
        "if df.duplicated().sum()==0:\n",
        "    print('Duplicates Removed')\n",
        "else:\n",
        "    print('there is still duplicates')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_1E3jsXyDsb"
      },
      "outputs": [],
      "source": [
        "colors=['#AB47BC','#6495ED']\n",
        "plt.pie(df['sentiment'].value_counts(),labels=['Positive','Negative'],autopct='%.2f%%',explode=[0.01,0.01],colors=colors);\n",
        "plt.title('Distribution of target')\n",
        "plt.ylabel('Sentiment');\n",
        "#data still balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhvrkkDZyE4I"
      },
      "outputs": [],
      "source": [
        "df['text_word_count']=df['review'].apply(lambda x:len(x.split()))\n",
        "\n",
        "numerical_feature_cols=['text_word_count']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWKW7AYDyGbV"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(20,3))\n",
        "for i,col in enumerate(numerical_feature_cols):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    sns.histplot(data=df,x=col,bins=50,color='#6495ED')\n",
        "    plt.title(f\"Distribution of Various word counts\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cE7rPTnyOLz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,3))\n",
        "for i,col in enumerate(numerical_feature_cols):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    sns.histplot(data=df,x=col,hue='sentiment',bins=50)\n",
        "    plt.title(f\"Distribution of Various word counts with respect to target\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhZe_k-EiMti"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjr6Z_dmyPzr"
      },
      "outputs": [],
      "source": [
        "#Setting English stopwords\n",
        "stopword_list=nltk.corpus.stopwords.words('english')\n",
        "stop=set(stopwords.words('english'))\n",
        "#Tokenization of text\n",
        "tokenizer=ToktokTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNi-QRvtyRrT"
      },
      "outputs": [],
      "source": [
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "#Removing Emails\n",
        "def remove_Emails(text):\n",
        "    pattern=r'\\S+@\\S+'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "\n",
        "#Removing URLS\n",
        "def remove_URLS(text):\n",
        "    pattern=r'http\\S+'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "\n",
        "#Removing special characters and white characters and punctuations\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "\n",
        "#Removing numbers\n",
        "def remove_numbers(text):\n",
        "    pattern = r'\\d+'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "#LOWERCASE\n",
        "def lowercase_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "#Stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps=nltk.porter.PorterStemmer()\n",
        "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "#removing the stopwords and conver to lower case\n",
        "def remove_stopwords(text, is_lower_case=True):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "#Removing br word\n",
        "def br_html(text):\n",
        "    pattern=r'br'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW-TSeAdyTKj"
      },
      "outputs": [],
      "source": [
        "df['review']=df['review'].apply(denoise_text)\n",
        "df['review']=df['review'].apply(remove_URLS)\n",
        "df['review']=df['review'].apply(remove_Emails)\n",
        "df['review']=df['review'].apply(remove_special_characters)\n",
        "df['review']=df['review'].apply(remove_numbers)\n",
        "df['review']=df['review'].apply(lowercase_text)\n",
        "\n",
        "#let's see how data is looklike after these prosessing\n",
        "random_index=random.randint(0,df.shape[0]-3)\n",
        "for row in df[['review','sentiment']][random_index:random_index+3].itertuples():\n",
        "    _,text,label=row\n",
        "    class_name=\"Positive\"\n",
        "    if label==0:\n",
        "        class_name=\"Negative\"\n",
        "    display(HTML(f\"<h5><b style='color:red'>Text: </b>{text}</h5>\"))\n",
        "    display(HTML(f\"<h5><b style='color:red'>Target: </b>{class_name}<br><hr></h5>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4LR8BT7yU1j"
      },
      "outputs": [],
      "source": [
        "df['review']=df['review'].apply(br_html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsz1ZYl8yWnZ"
      },
      "outputs": [],
      "source": [
        "df['review'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUzrVYnUyX5T"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(tag):\n",
        "    \"\"\"\n",
        "    Map POS tag to first character used by WordNetLemmatizer\n",
        "    \"\"\"\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # default to noun if POS tag is unknown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY3-gIdYyeXT"
      },
      "outputs": [],
      "source": [
        "def preprocess_review(review):\n",
        "    \"\"\"\n",
        "    Preprocess a single movie review by tokenizing and lemmatizing\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # Tokenize the review into words\n",
        "    words = nltk.word_tokenize(review)\n",
        "\n",
        "    # Tag the words with their part-of-speech (POS) tags\n",
        "    tagged_words = nltk.pos_tag(words)\n",
        "\n",
        "    # Lemmatize the words using their POS tags\n",
        "    lemmatized_words = []\n",
        "    for word, tag in tagged_words:\n",
        "        pos = get_wordnet_pos(tag)\n",
        "        lemmatized_word = lemmatizer.lemmatize(word, pos=pos)\n",
        "        lemmatized_words.append(lemmatized_word)\n",
        "\n",
        "    # Join the lemmatized words back into a single string\n",
        "    preprocessed_review = ' '.join(lemmatized_words)\n",
        "\n",
        "    return preprocessed_review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVIM9Glhyfje"
      },
      "outputs": [],
      "source": [
        "print ('BEFORE (remove_stopwords).. \\n',df['review'][2])\n",
        "df['review_lem']=df['review'].apply(preprocess_review)\n",
        "print ('AFTER (remove_stopwords) .. \\n',df['review_lem'][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj2wn7Vlyg5r"
      },
      "outputs": [],
      "source": [
        "print ('BEFORE (remove_stopwords).. \\n',df['review_lem'][2])\n",
        "df['review_lem']=df['review_lem'].apply(remove_stopwords)\n",
        "print ('AFTER (remove_stopwords) .. \\n',df['review_lem'][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF0dgmHiyid3"
      },
      "outputs": [],
      "source": [
        "df.to_csv('IMDB_Prosessed_V1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDnNjfEXykGu"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('IMDB_Prosessed_V1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgEwk2h7ylHv"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtveVft_yl-A"
      },
      "outputs": [],
      "source": [
        "#parameter for OOV tokens  is set to out of vocab and padding\n",
        "oov_tok = \"<OOV>\"\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "max_seq_length = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geygl_spyn1N"
      },
      "outputs": [],
      "source": [
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words=10000,oov_token=oov_tok)\n",
        "\n",
        "# Generate the word index dictionary\n",
        "tokenizer.fit_on_texts(df['review_lem'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwGveyD7yore"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['review_lem'], df['sentiment'].map({'positive': 1, 'negative': 0}), test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et5XkLTIyqFp"
      },
      "outputs": [],
      "source": [
        "# Generate and pad the training sequences\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0XQJddXyq-p"
      },
      "outputs": [],
      "source": [
        "x_train = pad_sequences(x_train, maxlen=max_seq_length, padding=padding_type, truncating=trunc_type)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGEmxAGEyr8W"
      },
      "outputs": [],
      "source": [
        "sentences = [review.split() for review in df['review_lem']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S3ccMo7ytmm"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=8,sg=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LVnG06Zyuky"
      },
      "outputs": [],
      "source": [
        "model.save('imdb_word2vec.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsEVnoWlyvgs"
      },
      "outputs": [],
      "source": [
        "# Load the trained Word2Vec model\n",
        "model = gensim.models.Word2Vec.load('imdb_word2vec.model')\n",
        "\n",
        "# Test the model on some sample words\n",
        "word1 = 'good'\n",
        "word2 = 'bad'\n",
        "word3 = 'movie'\n",
        "word4 = 'action'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QjuARP6yxLe"
      },
      "outputs": [],
      "source": [
        "# Get the most similar words to word1 and word2\n",
        "print(\"Most similar words to '{}' and '{}' are:\".format(word1, word2))\n",
        "print(model.wv.most_similar(positive=[word1], negative=[word2]))\n",
        "\n",
        "# Check if word3 is present in the vocabulary\n",
        "if word3 in model.wv.key_to_index:\n",
        "    print(\"Word '{}' is present in the vocabulary\".format(word3))\n",
        "else:\n",
        "    print(\"Word '{}' is not present in the vocabulary\".format(word3))\n",
        "\n",
        "# Check if word4 is present in the vocabulary\n",
        "if word4 in model.wv.key_to_index:\n",
        "    print(\"Word '{}' is present in the vocabulary\".format(word4))\n",
        "else:\n",
        "    print(\"Word '{}' is not present in thevocabulary\".format(word4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1IqqN1_yyYS"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(model.wv.key_to_index)\n",
        "embedding_dim = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEcfyXe_y0aS"
      },
      "outputs": [],
      "source": [
        "# Generate word embeddings from the model\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for i, word in enumerate(model.wv.index_to_key):\n",
        "    embedding_matrix[i] = model.wv[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R1vOdDuy2di"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "with tf.device('/GPU:0'):\n",
        "    model_lstm = Sequential ([\n",
        "            Embedding(input_dim=vocab_size,output_dim=embedding_dim,weights=[embedding_matrix],trainable=False),\n",
        "            Bidirectional(LSTM(100, return_sequences=True, dropout=0.35, recurrent_dropout=0.35)),\n",
        "            Conv1D(100, 5, activation='relu'),\n",
        "            GlobalMaxPooling1D(),\n",
        "            Dense(100, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "# Set the training parameters\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model_lstm.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
        "    # Print the model summary\n",
        "# model_lstm.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak9hbVB-y3f_"
      },
      "outputs": [],
      "source": [
        "model_lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6mjMyAyy4nw"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model_lstm,show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz9SttM5y6HD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "# create a TensorBoard callback\n",
        "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG-lWsbd4UIP"
      },
      "outputs": [],
      "source": [
        "# Monitor memory usage during training\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def print_memory_usage():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(f\"GPU memory usage: {process.memory_info().vms / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUmf3UNgy7hT",
        "outputId": "5cb809bb-d24f-4cce-df37-41f31c62ec56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory usage: 15.89 GB\n",
            "363/930 [==========>...................] - ETA: 7:00:49 - loss: 0.6097 - accuracy: 0.6596"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "from sklearn.metrics import f1_score\n",
        "with tf.device('/GPU:0'):\n",
        "    early_stopping = EarlyStopping(patience=3)\n",
        "    print_memory_usage()\n",
        "    # Reduce the batch size or the number of epochs to reduce memory usage.\n",
        "    # Reduce the batch size or the number of epochs to reduce memory usage.\n",
        "    history_lstm = model_lstm.fit(x_train, y_train, epochs=1, batch_size=32, verbose=1, validation_split=0.25, callbacks=[early_stopping, tensorboard_callback])\n",
        "    print_memory_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWxvVMiUy8ZM"
      },
      "outputs": [],
      "source": [
        "_, accuracy = model_lstm.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTETx5xay_JT"
      },
      "outputs": [],
      "source": [
        "y_pred = model_lstm.predict(x_test)\n",
        "# Convert the predicted probabilities to binary labels\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "# Compute the F1 score\n",
        "f1 = f1_score(validation_labels, y_pred)\n",
        "# Print the F1 score\n",
        "print(\"F1 score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUw5og2YzAe4"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curves(history):\n",
        "\n",
        "    '''\n",
        "      returns seperate loss curves for training and validation metrics\n",
        "    '''\n",
        "\n",
        "    train_loss=history.history['loss']\n",
        "    val_loss=history.history['val_loss']\n",
        "\n",
        "    train_accuracy=history.history['accuracy']\n",
        "    val_accuracy=history.history['val_accuracy']\n",
        "\n",
        "    epochs=range(1,len(history.history['loss'])+1)\n",
        "    plt.figure(figsize=(20,5))\n",
        "\n",
        "    # plot loss data\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs,train_loss,label=\"training_loss\")\n",
        "    plt.plot(epochs,val_loss,label=\"validation_loss\")\n",
        "    plt.title(\"Loss curves\",size=20)\n",
        "    plt.xlabel('epochs',size=20)\n",
        "    plt.ylabel('loss',size=20)\n",
        "    plt.legend(fontsize=15);\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "    # plot accuracy data\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs,train_accuracy,label=\"training_acc\")\n",
        "    plt.plot(epochs,val_accuracy,label=\"validation_acc\")\n",
        "    plt.title(\"Accuracy curves\",size=20)\n",
        "    plt.xlabel('epochs',size=20)\n",
        "    plt.ylabel('Accuracy',size=20)\n",
        "    plt.tight_layout()\n",
        "    plt.legend(fontsize=15);\n",
        "\n",
        "\n",
        "\n",
        "    plt.title('Model Performance Curves')\n",
        "\n",
        "\n",
        "plot_loss_curves(history_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwd7GdMrzCSc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
