{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU3HO2eTowWH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch torchtext torchdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "from collections import Counter, OrderedDict\n",
        "from dataclasses import dataclass\n",
        "from time import monotonic\n",
        "from typing import Dict, List, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from scipy.spatial.distance import cosine\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import WikiText103\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "ZJS1M4c-oxn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    # gets the data\n",
        "    train_iter = WikiText103(split='train')\n",
        "    train_iter = to_map_style_dataset(train_iter)\n",
        "    valid_iter = WikiText103(split='test')\n",
        "    valid_iter = to_map_style_dataset(valid_iter)\n",
        "\n",
        "    return train_iter, valid_iter"
      ],
      "metadata": {
        "id": "shxXAotV4v5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Word2VecParams:\n",
        "\n",
        "    # skipgram parameters\n",
        "    MIN_FREQ = 50\n",
        "    SKIPGRAM_N_WORDS = 8\n",
        "    T = 85\n",
        "    NEG_SAMPLES = 50\n",
        "    NS_ARRAY_LEN = 5_000_000\n",
        "    SPECIALS = \"<unk>\"\n",
        "    TOKENIZER = 'basic_english'\n",
        "\n",
        "    # network parameters\n",
        "    BATCH_SIZE = 100\n",
        "    EMBED_DIM = 300\n",
        "    EMBED_MAX_NORM = None\n",
        "    N_EPOCHS = 5\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    CRITERION = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "wKZNNoSto6GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    def __init__(self, list, specials):\n",
        "        self.stoi = {v[0]:(k, v[1]) for k, v in enumerate(list)}\n",
        "        self.itos = {k:(v[0], v[1]) for k, v in enumerate(list)}\n",
        "        self._specials = specials[0]\n",
        "        self.total_tokens = np.nansum(\n",
        "            [f for _, (_, f) in self.stoi.items()]\n",
        "            , dtype=int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.stoi) - 1\n",
        "\n",
        "    def get_index(self, word: Union[str, List]):\n",
        "        if isinstance(word, str):\n",
        "            if word in self.stoi:\n",
        "                return self.stoi.get(word)[0]\n",
        "            else:\n",
        "                return self.stoi.get(self._specials)[0]\n",
        "        elif isinstance(word, list):\n",
        "            res = []\n",
        "            for w in word:\n",
        "                if w in self.stoi:\n",
        "                    res.append(self.stoi.get(w)[0])\n",
        "                else:\n",
        "                    res.append(self.stoi.get(self._specials)[0])\n",
        "            return res\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Word {word} is not a string or a list of strings.\"\n",
        "                )\n",
        "\n",
        "\n",
        "    def get_freq(self, word: Union[str, List]):\n",
        "        if isinstance(word, str):\n",
        "            if word in self.stoi:\n",
        "                return self.stoi.get(word)[1]\n",
        "            else:\n",
        "                return self.stoi.get(self._specials)[1]\n",
        "        elif isinstance(word, list):\n",
        "            res = []\n",
        "            for w in word:\n",
        "                if w in self.stoi:\n",
        "                    res.append(self.stoi.get(w)[1])\n",
        "                else:\n",
        "                    res.append(self.stoi.get(self._specials)[1])\n",
        "            return res\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Word {word} is not a string or a list of strings.\"\n",
        "                )\n",
        "\n",
        "\n",
        "    def lookup_token(self, token: Union[int, List]):\n",
        "        if isinstance(token, (int, np.int64)):\n",
        "            if token in self.itos:\n",
        "                return self.itos.get(token)[0]\n",
        "            else:\n",
        "                raise ValueError(f\"Token {token} not in vocabulary\")\n",
        "        elif isinstance(token, list):\n",
        "            res = []\n",
        "            for t in token:\n",
        "                if t in self.itos:\n",
        "                    res.append(self.itos.get(token)[0])\n",
        "                else:\n",
        "                    raise ValueError(f\"Token {t} is not a valid index.\")\n",
        "            return res"
      ],
      "metadata": {
        "id": "FB5Kh3LCqBDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(iterator, tokenizer):\n",
        "    r = re.compile('[a-z1-9]')\n",
        "    for text in iterator:\n",
        "        res = tokenizer(text)\n",
        "        res = list(filter(r.match, res))\n",
        "        yield res\n",
        "\n",
        "def vocab(ordered_dict: Dict, min_freq: int = 1, specials: str = '<unk>'):\n",
        "    tokens = []\n",
        "    # Save room for special tokens\n",
        "    for token, freq in ordered_dict.items():\n",
        "        if freq >= min_freq:\n",
        "            tokens.append((token, freq))\n",
        "\n",
        "    specials = (specials, np.nan)\n",
        "    tokens[0] = specials\n",
        "\n",
        "    return Vocab(tokens, specials)\n",
        "\n",
        "def pipeline(word, vocab, tokenizer):\n",
        "    return vocab(tokenizer(word))\n",
        "\n",
        "def build_vocab(\n",
        "        iterator,\n",
        "        tokenizer,\n",
        "        params: Word2VecParams,\n",
        "        max_tokens: Optional[int] = None,\n",
        "    ):\n",
        "    counter = Counter()\n",
        "    for tokens in yield_tokens(iterator, tokenizer):\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # First sort by descending frequency, then lexicographically\n",
        "    sorted_by_freq_tuples = sorted(\n",
        "        counter.items(), key=lambda x: (-x[1], x[0])\n",
        "        )\n",
        "\n",
        "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "\n",
        "    word_vocab = vocab(\n",
        "        ordered_dict, min_freq=params.MIN_FREQ, specials=params.SPECIALS\n",
        "        )\n",
        "    return word_vocab\n"
      ],
      "metadata": {
        "id": "iJp-0yudpFfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGrams:\n",
        "    def __init__(self, vocab: Vocab, params: Word2VecParams, tokenizer):\n",
        "        self.vocab = vocab\n",
        "        self.params = params\n",
        "        self.t = self._t()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.discard_probs = self._create_discard_dict()\n",
        "\n",
        "    def _t(self):\n",
        "        freq_list = []\n",
        "        for _, (_, freq) in list(self.vocab.stoi.items())[1:]:\n",
        "            freq_list.append(freq/self.vocab.total_tokens)\n",
        "        return np.percentile(freq_list, self.params.T)\n",
        "\n",
        "\n",
        "    def _create_discard_dict(self):\n",
        "        discard_dict = {}\n",
        "        for _, (word, freq) in self.vocab.stoi.items():\n",
        "            dicard_prob = 1-np.sqrt(\n",
        "                self.t / (freq/self.vocab.total_tokens + self.t))\n",
        "            discard_dict[word] = dicard_prob\n",
        "        return discard_dict\n",
        "\n",
        "\n",
        "    def collate_skipgram(self, batch):\n",
        "        batch_input, batch_output  = [], []\n",
        "        for text in batch:\n",
        "            text_tokens = self.vocab.get_index(self.tokenizer(text))\n",
        "\n",
        "            if len(text_tokens) < self.params.SKIPGRAM_N_WORDS * 2 + 1:\n",
        "                continue\n",
        "\n",
        "            for idx in range(len(text_tokens) - self.params.SKIPGRAM_N_WORDS*2\n",
        "                ):\n",
        "                token_id_sequence = text_tokens[\n",
        "                    idx : (idx + self.params.SKIPGRAM_N_WORDS * 2 + 1)\n",
        "                    ]\n",
        "                input_ = token_id_sequence.pop(self.params.SKIPGRAM_N_WORDS)\n",
        "                outputs = token_id_sequence\n",
        "\n",
        "                prb = random.random()\n",
        "                del_pair = self.discard_probs.get(input_)\n",
        "                if input_==0 or del_pair >= prb:\n",
        "                    continue\n",
        "                else:\n",
        "                    for output in outputs:\n",
        "                        prb = random.random()\n",
        "                        del_pair = self.discard_probs.get(output)\n",
        "                        if output==0 or del_pair >= prb:\n",
        "                            continue\n",
        "                        else:\n",
        "                            batch_input.append(input_)\n",
        "                            batch_output.append(output)\n",
        "\n",
        "        batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "        batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "\n",
        "        return batch_input, batch_output"
      ],
      "metadata": {
        "id": "V-5vtReKtlg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NegativeSampler:\n",
        "    def __init__(self, vocab: Vocab, ns_exponent: float, ns_array_len: int):\n",
        "        self.vocab = vocab\n",
        "        self.ns_exponent = ns_exponent\n",
        "        self.ns_array_len = ns_array_len\n",
        "        self.ns_array = self._create_negative_sampling()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ns_array)\n",
        "\n",
        "    def _create_negative_sampling(self):\n",
        "\n",
        "        frequency_dict = {word:freq**(self.ns_exponent) \\\n",
        "                          for _,(word, freq) in\n",
        "                          list(self.vocab.stoi.items())[1:]}\n",
        "        frequency_dict_scaled = {\n",
        "            word:\n",
        "            max(1,int((freq/self.vocab.total_tokens)*self.ns_array_len))\n",
        "            for word, freq in frequency_dict.items()\n",
        "            }\n",
        "        ns_array = []\n",
        "        for word, freq in tqdm(frequency_dict_scaled.items()):\n",
        "            ns_array = ns_array + [word]*freq\n",
        "        return ns_array\n",
        "\n",
        "    def sample(self,n_batches: int=1, n_samples: int=1):\n",
        "        samples = []\n",
        "        for _ in range(n_batches):\n",
        "            samples.append(random.sample(self.ns_array, n_samples))\n",
        "        samples = torch.as_tensor(np.array(samples))\n",
        "        return samples\n"
      ],
      "metadata": {
        "id": "0mTWsnXTxTXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab: Vocab, params: Word2VecParams):\n",
        "        super().__init__()\n",
        "        self.vocab = vocab\n",
        "        self.t_embeddings = nn.Embedding(\n",
        "            self.vocab.__len__()+1,\n",
        "            params.EMBED_DIM,\n",
        "            max_norm=params.EMBED_MAX_NORM\n",
        "            )\n",
        "        self.c_embeddings = nn.Embedding(\n",
        "            self.vocab.__len__()+1,\n",
        "            params.EMBED_DIM,\n",
        "            max_norm=params.EMBED_MAX_NORM\n",
        "            )\n",
        "\n",
        "    def forward(self, inputs, context):\n",
        "        # getting embeddings for target & reshaping\n",
        "        target_embeddings = self.t_embeddings(inputs)\n",
        "        n_examples = target_embeddings.shape[0]\n",
        "        n_dimensions = target_embeddings.shape[1]\n",
        "        target_embeddings = target_embeddings.view(n_examples, 1, n_dimensions)\n",
        "\n",
        "        # get embeddings for context labels & reshaping\n",
        "        # Allows us to do a bunch of matrix multiplications\n",
        "        context_embeddings = self.c_embeddings(context)\n",
        "        # * This transposes each batch\n",
        "        context_embeddings = context_embeddings.permute(0,2,1)\n",
        "\n",
        "        # * custom linear layer\n",
        "        dots = target_embeddings.bmm(context_embeddings)\n",
        "        dots = dots.view(dots.shape[0], dots.shape[2])\n",
        "        return dots\n",
        "\n",
        "    def normalize_embeddings(self):\n",
        "        embeddings = list(self.t_embeddings.parameters())[0]\n",
        "        embeddings = embeddings.cpu().detach().numpy()\n",
        "        norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
        "        norms = norms.reshape(norms.shape[0], 1)\n",
        "        return embeddings / norms\n",
        "\n",
        "    def get_similar_words(self, word, n):\n",
        "        word_id = self.vocab.get_index(word)\n",
        "        if word_id == 0:\n",
        "            print(\"Out of vocabulary word\")\n",
        "            return\n",
        "\n",
        "        embedding_norms = self.normalize_embeddings()\n",
        "        word_vec = embedding_norms[word_id]\n",
        "        word_vec = np.reshape(word_vec, (word_vec.shape[0], 1))\n",
        "        dists = np.matmul(embedding_norms, word_vec).flatten()\n",
        "        topN_ids = np.argsort(-dists)[1 : n + 1]\n",
        "\n",
        "        topN_dict = {}\n",
        "        for sim_word_id in topN_ids:\n",
        "            sim_word = self.vocab.lookup_token(sim_word_id)\n",
        "            topN_dict[sim_word] = dists[sim_word_id]\n",
        "        return topN_dict\n",
        "\n",
        "    def get_similarity(self, word1, word2):\n",
        "        idx1 = self.vocab.get_index(word1)\n",
        "        idx2 = self.vocab.get_index(word2)\n",
        "        if idx1 == 0 or idx2 == 0:\n",
        "            print(\"One or both words are out of vocabulary\")\n",
        "            return\n",
        "\n",
        "        embedding_norms = self.normalize_embeddings()\n",
        "        word1_vec, word2_vec = embedding_norms[idx1], embedding_norms[idx2]\n",
        "\n",
        "        return cosine(word1_vec, word2_vec)"
      ],
      "metadata": {
        "id": "a-Xjg40wvtag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model: Model, params: Word2VecParams, optimizer,\n",
        "                vocab: Vocab, train_iter, valid_iter, skipgrams: SkipGrams):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.vocab = vocab\n",
        "        self.train_iter = train_iter\n",
        "        self.valid_iter = valid_iter\n",
        "        self.skipgrams = skipgrams\n",
        "        self.params = params\n",
        "\n",
        "        self.epoch_train_mins = {}\n",
        "        self.loss = {\"train\": [], \"valid\": []}\n",
        "\n",
        "        # sending all to device\n",
        "        self.model.to(self.params.DEVICE)\n",
        "        self.params.CRITERION.to(self.params.DEVICE)\n",
        "\n",
        "        self.negative_sampler = NegativeSampler(\n",
        "            vocab=self.vocab, ns_exponent=.75,\n",
        "            ns_array_len=self.params.NS_ARRAY_LEN\n",
        "            )\n",
        "        self.testwords = ['love', 'hurricane', 'military', 'army']\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        self.test_testwords()\n",
        "        for epoch in range(self.params.N_EPOCHS):\n",
        "            # Generate Dataloaders\n",
        "            self.train_dataloader = DataLoader(\n",
        "                self.train_iter,\n",
        "                batch_size=self.params.BATCH_SIZE,\n",
        "                shuffle=False,\n",
        "                collate_fn=self.skipgrams.collate_skipgram\n",
        "            )\n",
        "            self.valid_dataloader = DataLoader(\n",
        "                self.valid_iter,\n",
        "                batch_size=self.params.BATCH_SIZE,\n",
        "                shuffle=False,\n",
        "                collate_fn=self.skipgrams.collate_skipgram\n",
        "            )\n",
        "            # training the model\n",
        "            st_time = monotonic()\n",
        "            self._train_epoch()\n",
        "            self.epoch_train_mins[epoch] = round((monotonic()-st_time)/60, 1)\n",
        "\n",
        "            # validating the model\n",
        "            self._validate_epoch()\n",
        "            print(f\"\"\"Epoch: {epoch+1}/{self.params.N_EPOCHS}\\n\"\"\",\n",
        "            f\"\"\"    Train Loss: {self.loss['train'][-1]:.2}\\n\"\"\",\n",
        "            f\"\"\"    Valid Loss: {self.loss['valid'][-1]:.2}\\n\"\"\",\n",
        "            f\"\"\"    Training Time (mins): {self.epoch_train_mins.get(epoch)}\"\"\"\n",
        "            \"\"\"\\n\"\"\"\n",
        "            )\n",
        "            self.test_testwords()\n",
        "\n",
        "\n",
        "    def _train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = []\n",
        "\n",
        "        for i, batch_data in enumerate(self.train_dataloader, 1):\n",
        "            if len(batch_data[0]) == 0:\n",
        "                continue\n",
        "            inputs = batch_data[0].to(self.params.DEVICE)\n",
        "            pos_labels = batch_data[1].to(self.params.DEVICE)\n",
        "            neg_labels = self.negative_sampler.sample(\n",
        "                pos_labels.shape[0], self.params.NEG_SAMPLES\n",
        "                )\n",
        "            neg_labels = neg_labels.to(self.params.DEVICE)\n",
        "            context = torch.cat(\n",
        "                [pos_labels.view(pos_labels.shape[0], 1),\n",
        "                neg_labels], dim=1\n",
        "              )\n",
        "\n",
        "            # building the targets tensor\n",
        "            y_pos = torch.ones((pos_labels.shape[0], 1))\n",
        "            y_neg = torch.zeros((neg_labels.shape[0], neg_labels.shape[1]))\n",
        "            y = torch.cat([y_pos, y_neg], dim=1).to(self.params.DEVICE)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs, context)\n",
        "            loss = self.params.CRITERION(outputs, y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "        epoch_loss = np.mean(running_loss)\n",
        "\n",
        "        self.loss['train'].append(epoch_loss)\n",
        "\n",
        "    def _validate_epoch(self):\n",
        "        self.model.eval()\n",
        "        running_loss = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch_data in enumerate(self.valid_dataloader, 1):\n",
        "                if len(batch_data[0]) == 0:\n",
        "                    continue\n",
        "                inputs = batch_data[0].to(self.params.DEVICE)\n",
        "                pos_labels = batch_data[1].to(self.params.DEVICE)\n",
        "                neg_labels = self.negative_sampler.sample(\n",
        "                    pos_labels.shape[0], self.params.NEG_SAMPLES\n",
        "                    ).to(self.params.DEVICE)\n",
        "                context = torch.cat(\n",
        "                    [pos_labels.view(pos_labels.shape[0], 1),\n",
        "                    neg_labels], dim=1\n",
        "                  )\n",
        "\n",
        "\n",
        "                # building the targets tensor\n",
        "                y_pos = torch.ones((pos_labels.shape[0], 1))\n",
        "                y_neg = torch.zeros((neg_labels.shape[0], neg_labels.shape[1]))\n",
        "                y = torch.cat([y_pos, y_neg], dim=1).to(self.params.DEVICE)\n",
        "\n",
        "                preds = self.model(inputs, context).to(self.params.DEVICE)\n",
        "                loss = self.params.CRITERION(preds, y)\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "            epoch_loss = np.mean(running_loss)\n",
        "            self.loss['valid'].append(epoch_loss)\n",
        "\n",
        "    def test_testwords(self, n: int = 5):\n",
        "        for word in self.testwords:\n",
        "            print(word)\n",
        "            nn_words = self.model.get_similar_words(word, n)\n",
        "            for w, sim in nn_words.items():\n",
        "                print(f\"{w} ({sim:.3})\", end=' ')\n",
        "            print('\\n')"
      ],
      "metadata": {
        "id": "mMx67oLYwS9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = Word2VecParams()\n",
        "train_iter, valid_iter = get_data()\n",
        "tokenizer = get_tokenizer(params.TOKENIZER)\n",
        "vocab = build_vocab(train_iter, tokenizer, params)\n",
        "skip_gram = SkipGrams(vocab=vocab, params=params, tokenizer=tokenizer)\n",
        "model = Model(vocab=vocab, params=params).to(params.DEVICE)\n",
        "optimizer = torch.optim.Adam(params = model.parameters())"
      ],
      "metadata": {
        "id": "q6DtwPHfqRVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        params=params,\n",
        "        optimizer=optimizer,\n",
        "        train_iter=train_iter,\n",
        "        valid_iter=valid_iter,\n",
        "        vocab=vocab,\n",
        "        skipgrams=skip_gram\n",
        "    )\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63ZAFXnOxi32",
        "outputId": "c4d18e9a-2e63-4ea4-a690-fc236ce92485"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 49331/49331 [01:14<00:00, 663.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5\n",
            "     Train Loss: 0.96\n",
            "     Valid Loss: 0.18\n",
            "     Training Time (mins): 43.0\n",
            "\n",
            "love\n",
            "music (0.44) her (0.44) has (0.43) s (0.424) wrote (0.424) \n",
            "\n",
            "hurricane\n",
            "landfall (0.287) changing (0.28) rapidly (0.279) steadily (0.276) winds (0.275) \n",
            "\n",
            "military\n",
            "by (0.457) for (0.45) although (0.449) was (0.449) any (0.444) \n",
            "\n",
            "army\n",
            "british (0.641) were (0.605) only (0.605) during (0.604) in (0.604) \n",
            "\n",
            "Epoch: 2/5\n",
            "     Train Loss: 0.14\n",
            "     Valid Loss: 0.11\n",
            "     Training Time (mins): 42.4\n",
            "\n",
            "love\n",
            "me (0.851) my (0.849) said (0.845) good (0.838) saying (0.832) \n",
            "\n",
            "hurricane\n",
            "storm (0.676) tropical (0.64) landfall (0.629) winds (0.625) rapidly (0.589) \n",
            "\n",
            "military\n",
            "war (0.852) army (0.839) support (0.829) forces (0.82) supported (0.819) \n",
            "\n",
            "army\n",
            "forces (0.896) troops (0.891) war (0.854) captured (0.839) military (0.839) \n",
            "\n",
            "Epoch: 3/5\n",
            "     Train Loss: 0.1\n",
            "     Valid Loss: 0.096\n",
            "     Training Time (mins): 42.5\n",
            "\n",
            "love\n",
            "girl (0.825) woman (0.787) me (0.782) herself (0.781) song (0.773) \n",
            "\n",
            "hurricane\n",
            "storm (0.787) tropical (0.745) landfall (0.736) cyclone (0.71) winds (0.685) \n",
            "\n",
            "military\n",
            "army (0.783) war (0.773) forces (0.756) government (0.74) troops (0.731) \n",
            "\n",
            "army\n",
            "troops (0.859) forces (0.848) command (0.796) military (0.783) commanded (0.779) \n",
            "\n",
            "Epoch: 4/5\n",
            "     Train Loss: 0.092\n",
            "     Valid Loss: 0.091\n",
            "     Training Time (mins): 42.2\n",
            "\n",
            "love\n",
            "girl (0.789) me (0.768) my (0.762) you (0.746) lover (0.743) \n",
            "\n",
            "hurricane\n",
            "storm (0.825) tropical (0.787) landfall (0.781) cyclone (0.761) winds (0.709) \n",
            "\n",
            "military\n",
            "army (0.764) forces (0.744) troops (0.701) war (0.701) leadership (0.697) \n",
            "\n",
            "army\n",
            "troops (0.836) forces (0.836) commanded (0.773) military (0.764) corps (0.743) \n",
            "\n",
            "Epoch: 5/5\n",
            "     Train Loss: 0.089\n",
            "     Valid Loss: 0.09\n",
            "     Training Time (mins): 42.2\n",
            "\n",
            "love\n",
            "girl (0.763) my (0.728) me (0.723) lover (0.722) you (0.709) \n",
            "\n",
            "hurricane\n",
            "storm (0.85) tropical (0.805) landfall (0.791) cyclone (0.781) intensity (0.717) \n",
            "\n",
            "military\n",
            "army (0.717) forces (0.674) officers (0.673) leadership (0.662) soldiers (0.662) \n",
            "\n",
            "army\n",
            "troops (0.823) forces (0.804) command (0.761) corps (0.758) commanded (0.753) \n",
            "\n"
          ]
        }
      ]
    }
  ]
}