{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AP92Zd7N-G24"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# upload your kaggle.json file here\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "EnCBaOZN-Ri_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "-mf3qhEw-TCT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "lnVSK0ts-TlM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BFWLObR-VdQ",
        "outputId": "0530c997-5118-4ac8-a3cd-6db2fadc1009"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading massive-stock-news-analysis-db-for-nlpbacktests.zip to /content\n",
            " 99% 208M/210M [00:11<00:00, 22.7MB/s]\n",
            "100% 210M/210M [00:11<00:00, 18.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('massive-stock-news-analysis-db-for-nlpbacktests.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "0XTXZhGK_yhM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "yJZvlGer-iMF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('raw_partner_headlines.csv')"
      ],
      "metadata": {
        "id": "JCjQzXHD-s6Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5T3t3Gd_ACMD",
        "outputId": "55c2eb8a-b654-4dee-afdd-8c0ee1e08c03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                           headline  \\\n",
              "0           2  Agilent Technologies Announces Pricing of $5……...   \n",
              "1           3  Agilent (A) Gears Up for Q2 Earnings: What's i...   \n",
              "2           4  J.P. Morgan Asset Management Announces Liquida...   \n",
              "3           5  Pershing Square Capital Management, L.P. Buys ...   \n",
              "4           6  Agilent Awards Trilogy Sciences with a Golden ...   \n",
              "\n",
              "                                                 url  publisher  \\\n",
              "0  http://www.gurufocus.com/news/1153187/agilent-...  GuruFocus   \n",
              "1  http://www.zacks.com/stock/news/931205/agilent...      Zacks   \n",
              "2  http://www.gurufocus.com/news/1138923/jp-morga...  GuruFocus   \n",
              "3  http://www.gurufocus.com/news/1138704/pershing...  GuruFocus   \n",
              "4  http://www.gurufocus.com/news/1134012/agilent-...  GuruFocus   \n",
              "\n",
              "                  date stock  \n",
              "0  2020-06-01 00:00:00     A  \n",
              "1  2020-05-18 00:00:00     A  \n",
              "2  2020-05-15 00:00:00     A  \n",
              "3  2020-05-15 00:00:00     A  \n",
              "4  2020-05-12 00:00:00     A  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b7875115-d853-4634-8fc5-efe7b7bdb86d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>headline</th>\n",
              "      <th>url</th>\n",
              "      <th>publisher</th>\n",
              "      <th>date</th>\n",
              "      <th>stock</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Agilent Technologies Announces Pricing of $5……...</td>\n",
              "      <td>http://www.gurufocus.com/news/1153187/agilent-...</td>\n",
              "      <td>GuruFocus</td>\n",
              "      <td>2020-06-01 00:00:00</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>Agilent (A) Gears Up for Q2 Earnings: What's i...</td>\n",
              "      <td>http://www.zacks.com/stock/news/931205/agilent...</td>\n",
              "      <td>Zacks</td>\n",
              "      <td>2020-05-18 00:00:00</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>J.P. Morgan Asset Management Announces Liquida...</td>\n",
              "      <td>http://www.gurufocus.com/news/1138923/jp-morga...</td>\n",
              "      <td>GuruFocus</td>\n",
              "      <td>2020-05-15 00:00:00</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>Pershing Square Capital Management, L.P. Buys ...</td>\n",
              "      <td>http://www.gurufocus.com/news/1138704/pershing...</td>\n",
              "      <td>GuruFocus</td>\n",
              "      <td>2020-05-15 00:00:00</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>Agilent Awards Trilogy Sciences with a Golden ...</td>\n",
              "      <td>http://www.gurufocus.com/news/1134012/agilent-...</td>\n",
              "      <td>GuruFocus</td>\n",
              "      <td>2020-05-12 00:00:00</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7875115-d853-4634-8fc5-efe7b7bdb86d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b7875115-d853-4634-8fc5-efe7b7bdb86d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b7875115-d853-4634-8fc5-efe7b7bdb86d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3fc07cd9-6493-4763-8752-0211c1d2df86\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3fc07cd9-6493-4763-8752-0211c1d2df86')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3fc07cd9-6493-4763-8752-0211c1d2df86 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news = []\n",
        "for i, j in df.iterrows():\n",
        "    news.append(j['headline'])\n",
        "\n",
        "print(len(news))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDIncNNzADP9",
        "outputId": "af355100-d828-4d09-ffd2-ee10b4ef105e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1845559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxTq2LbZAb5C",
        "outputId": "8175742f-2cb2-461b-fa1d-d666f9c0deae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Agilent Technologies Announces Pricing of $5…… Million of Senior Notes']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(news)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7vSrWlKAeMc",
        "outputId": "c4f1ea1f-9fbb-4f9c-ccc5-a3ffec310b24"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1845559"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news = news[:109233]"
      ],
      "metadata": {
        "id": "6bqTQPMVAgKN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(news)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AJhX2f5AldA",
        "outputId": "369de7c9-f15c-4795-aaf6-fecbfa5608d5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109233"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir news"
      ],
      "metadata": {
        "id": "okwr_-O1F2b2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.join('/content/news', 'finance_news.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TruuanAzA5I4",
        "outputId": "aa66a438-27df-437f-deab-23a2e723661f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/news/finance_news.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/news/finance_news.txt', 'w')\n",
        "f.write('\\n'.join(news))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "y2DWRXIUBmAb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "\n",
        "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Load Dataset from File\n",
        "    \"\"\"\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = f.read()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n",
        "    \"\"\"\n",
        "    Preprocess Text Data\n",
        "    \"\"\"\n",
        "    text = load_data(dataset_path)\n",
        "\n",
        "    # Ignore notice, since we don't use it for analysing the data\n",
        "    text = text[81:]\n",
        "\n",
        "    token_dict = token_lookup()\n",
        "    for key, token in token_dict.items():\n",
        "        text = text.replace(key, ' {} '.format(token))\n",
        "\n",
        "    text = text.lower()\n",
        "    text = text.split()\n",
        "\n",
        "    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
        "    int_text = [vocab_to_int[word] for word in text]\n",
        "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))\n",
        "\n",
        "\n",
        "def load_preprocess():\n",
        "    \"\"\"\n",
        "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
        "    \"\"\"\n",
        "    return pickle.load(open('preprocess.p', mode='rb'))\n",
        "\n",
        "\n",
        "def save_model(filename, decoder):\n",
        "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
        "    torch.save(decoder, save_filename)\n",
        "\n",
        "\n",
        "def load_model(filename):\n",
        "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
        "    return torch.load(save_filename)"
      ],
      "metadata": {
        "id": "qniaAvXdA9on"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/news/finance_news.txt'\n",
        "text = load_data(data_dir)"
      ],
      "metadata": {
        "id": "qGEtCsR1Cv84"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view_line_range = (0, 10)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print('Dataset Stats')\n",
        "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
        "\n",
        "lines = text.split('\\n')\n",
        "print('Number of lines: {}'.format(len(lines)))\n",
        "word_count_line = [len(line.split()) for line in lines]\n",
        "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
        "\n",
        "print()\n",
        "print('The lines {} to {}:'.format(*view_line_range))\n",
        "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSmtIhW9C06o",
        "outputId": "633baa40-df69-4a6b-d041-4513b4ca1e33"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Stats\n",
            "Roughly the number of unique words: 58299\n",
            "Number of lines: 109233\n",
            "Average number of words in each line: 9.44242124632666\n",
            "\n",
            "The lines 0 to 10:\n",
            "Agilent Technologies Announces Pricing of $5…… Million of Senior Notes\n",
            "Agilent (A) Gears Up for Q2 Earnings: What's in the Cards?\n",
            "J.P. Morgan Asset Management Announces Liquidation of Six Exchange-Traded Funds\n",
            "Pershing Square Capital Management, L.P. Buys Agilent Technologies Inc, The Howard Hughes Corp, ...\n",
            "Agilent Awards Trilogy Sciences with a Golden Ticket at LabCentral\n",
            "Agilent Technologies Inc (A) CEO and President Michael R. Mcmullen Sold $–.4 million of Shares\n",
            "' Stocks Growing Their Earnings Fast\n",
            "Cypress Asset Management Inc Buys Verizon Communications Inc, United Parcel Service Inc, ...\n",
            "Hendley & Co Inc Buys American Electric Power Co Inc, Agilent Technologies Inc, Paychex ...\n",
            "Teacher Retirement System Of Texas Buys Hologic Inc, Vanguard Total Stock Market, Agilent ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def create_lookup_tables(text):\n",
        "    \"\"\"\n",
        "    Create lookup tables for vocabulary\n",
        "    :param text: The text of tv scripts split into words\n",
        "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    word_count = Counter(text)\n",
        "    sorted_vocab = sorted(word_count, key = word_count.get, reverse=True)\n",
        "    int_to_vocab = {ii:word for ii, word in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {word:ii for ii, word in int_to_vocab.items()}\n",
        "\n",
        "    # return tuple\n",
        "    return (vocab_to_int, int_to_vocab)\n"
      ],
      "metadata": {
        "id": "sGNjk2rnC3o1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token_lookup():\n",
        "    \"\"\"\n",
        "    Generate a dict to turn punctuation into a token.\n",
        "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    token = dict()\n",
        "    token['.'] = '<PERIOD>'\n",
        "    token[','] = '<COMMA>'\n",
        "    token['\"'] = 'QUOTATION_MARK'\n",
        "    token[';'] = 'SEMICOLON'\n",
        "    token['!'] = 'EXCLAIMATION_MARK'\n",
        "    token['?'] = 'QUESTION_MARK'\n",
        "    token['('] = 'LEFT_PAREN'\n",
        "    token[')'] = 'RIGHT_PAREN'\n",
        "    token['-'] = 'QUESTION_MARK'\n",
        "    token['\\n'] = 'NEW_LINE'\n",
        "    return token\n"
      ],
      "metadata": {
        "id": "v_3QW8bHDT_b"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
      ],
      "metadata": {
        "id": "_HhwdARZDWO5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"
      ],
      "metadata": {
        "id": "Yd0uEBVNDXji"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_on_gpu = torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "yQRfPp3eDYrI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def batch_data(words, sequence_length, batch_size):\n",
        "    \"\"\"\n",
        "    Batch the neural network data using DataLoader\n",
        "    :param words: The word ids of the TV scripts\n",
        "    :param sequence_length: The sequence length of each batch\n",
        "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
        "    :return: DataLoader with batched data\n",
        "    \"\"\"\n",
        "    # TODO: Implement function\n",
        "    n_batches = len(words)//batch_size\n",
        "    x, y = [], []\n",
        "    words = words[:n_batches*batch_size]\n",
        "\n",
        "    for ii in range(0, len(words)-sequence_length):\n",
        "        i_end = ii+sequence_length\n",
        "        batch_x = words[ii:ii+sequence_length]\n",
        "        x.append(batch_x)\n",
        "        batch_y = words[i_end]\n",
        "        y.append(batch_y)\n",
        "\n",
        "    data = TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n",
        "    data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    # return a dataloader\n",
        "    return data_loader\n"
      ],
      "metadata": {
        "id": "NLNy025KDa0I"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the PyTorch RNN Module\n",
        "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
        "        :param output_size: The number of output dimensions of the neural network\n",
        "        :param embedding_dim: The size of embeddings, should you choose to use them\n",
        "        :param hidden_dim: The size of the hidden layer outputs\n",
        "        :param dropout: dropout to add in between LSTM/GRU layers\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "        # TODO: Implement function\n",
        "\n",
        "        # define embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # define lstm layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "\n",
        "        # set class variables\n",
        "        self.vocab_size = vocab_size\n",
        "        self.output_size = output_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # define model layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation of the neural network\n",
        "        :param nn_input: The input to the neural network\n",
        "        :param hidden: The hidden state\n",
        "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
        "        \"\"\"\n",
        "        # TODO: Implement function\n",
        "        batch_size = x.size(0)\n",
        "        x=x.long()\n",
        "\n",
        "        # embedding and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        # stack up lstm layers\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        # dropout, fc layer and final sigmoid layer\n",
        "        out = self.fc(lstm_out)\n",
        "\n",
        "        # reshaping out layer to batch_size * seq_length * output_size\n",
        "        out = out.view(batch_size, -1, self.output_size)\n",
        "\n",
        "        # return last batch\n",
        "        out = out[:, -1]\n",
        "\n",
        "        # return one batch of output word scores and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        '''\n",
        "        Initialize the hidden state of an LSTM/GRU\n",
        "        :param batch_size: The batch_size of the hidden state\n",
        "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
        "        '''\n",
        "        # create 2 new zero tensors of size n_layers * batch_size * hidden_dim\n",
        "        weights = next(self.parameters()).data\n",
        "        if(train_on_gpu):\n",
        "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "\n",
        "        # initialize hidden state with zero weights, and move to GPU if available\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "JntD8_EmElaP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
        "    \"\"\"\n",
        "    Forward and backward propagation on the neural network\n",
        "    :param decoder: The PyTorch Module that holds the neural network\n",
        "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
        "    :param criterion: The PyTorch loss function\n",
        "    :param inp: A batch of input to the neural network\n",
        "    :param target: The target output for the batch of input\n",
        "    :return: The loss and the latest hidden state Tensor\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Implement Function\n",
        "\n",
        "    # move data to GPU, if available\n",
        "    if(train_on_gpu):\n",
        "        rnn.cuda()\n",
        "\n",
        "    # creating variables for hidden state to prevent back-propagation\n",
        "    # of historical states\n",
        "    h = tuple([each.data for each in hidden])\n",
        "\n",
        "    rnn.zero_grad()\n",
        "    # move inputs, targets to GPU\n",
        "    inputs, targets = inp.cuda(), target.cuda()\n",
        "\n",
        "    output, h = rnn(inputs, h)\n",
        "\n",
        "    loss = criterion(output, targets)\n",
        "\n",
        "    # perform backpropagation and optimization\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
        "    optimizer.step()\n",
        "\n",
        "    # return the loss over a batch and the hidden state produced by our model\n",
        "    return loss.item(), h\n"
      ],
      "metadata": {
        "id": "oYiKN7cXEoQi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
        "    batch_losses = []\n",
        "\n",
        "    rnn.train()\n",
        "\n",
        "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
        "    for epoch_i in range(1, n_epochs + 1):\n",
        "\n",
        "        # initialize hidden state\n",
        "        hidden = rnn.init_hidden(batch_size)\n",
        "\n",
        "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "\n",
        "            # make sure you iterate over completely full batches, only\n",
        "            n_batches = len(train_loader.dataset)//batch_size\n",
        "            if(batch_i > n_batches):\n",
        "                break\n",
        "\n",
        "            # forward, back prop\n",
        "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)\n",
        "            # record loss\n",
        "            batch_losses.append(loss)\n",
        "\n",
        "            # printing loss stats\n",
        "            if batch_i % show_every_n_batches == 0:\n",
        "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
        "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
        "                batch_losses = []\n",
        "\n",
        "    # returns a trained rnn\n",
        "    return rnn"
      ],
      "metadata": {
        "id": "RP9hbfnzEp_d"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data params\n",
        "# Sequence Length\n",
        "sequence_length = 10  # of words in a sequence\n",
        "# Batch Size\n",
        "batch_size = 128\n",
        "\n",
        "# data loader - do not change\n",
        "train_loader = batch_data(int_text, sequence_length, batch_size)"
      ],
      "metadata": {
        "id": "AfuCr6o1ErYx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "# Number of Epochs\n",
        "num_epochs = 10\n",
        "# Learning Rate\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Model parameters\n",
        "# Vocab size\n",
        "vocab_size = len(vocab_to_int)\n",
        "# Output size\n",
        "output_size = vocab_size\n",
        "# Embedding Dimension\n",
        "embedding_dim = 200\n",
        "# Hidden Dimension\n",
        "hidden_dim = 250\n",
        "# Number of RNN Layers\n",
        "n_layers = 2 # 4\n",
        "# # bidirectional op RNN layers\n",
        "# bi_lstm = True/False\n",
        "\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 500"
      ],
      "metadata": {
        "id": "6g0QZrxOEtDS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model and move to gpu if available\n",
        "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
        "if train_on_gpu:\n",
        "    rnn.cuda()\n",
        "\n",
        "# defining loss and optimization functions for training\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# training the model\n",
        "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
        "\n",
        "# saving the trained model\n",
        "save_model('./save/trained_rnn', trained_rnn)\n",
        "print('Model Trained and Saved')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XIJBj3-EyKo",
        "outputId": "a795da06-ff1a-4d70-dba8-0e0f825597f0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10 epoch(s)...\n",
            "Epoch:    1/10    Loss: 7.060120074272156\n",
            "\n",
            "Epoch:    1/10    Loss: 6.221924820899964\n",
            "\n",
            "Epoch:    1/10    Loss: 5.833479302406311\n",
            "\n",
            "Epoch:    1/10    Loss: 5.5864805402755735\n",
            "\n",
            "Epoch:    1/10    Loss: 5.3769022626876835\n",
            "\n",
            "Epoch:    1/10    Loss: 5.215042684555054\n",
            "\n",
            "Epoch:    1/10    Loss: 5.0929704885482785\n",
            "\n",
            "Epoch:    1/10    Loss: 5.028893412590027\n",
            "\n",
            "Epoch:    1/10    Loss: 4.933740097999573\n",
            "\n",
            "Epoch:    1/10    Loss: 4.826177488327026\n",
            "\n",
            "Epoch:    1/10    Loss: 4.7618762369155885\n",
            "\n",
            "Epoch:    1/10    Loss: 4.6982204675674435\n",
            "\n",
            "Epoch:    1/10    Loss: 4.636743475914002\n",
            "\n",
            "Epoch:    1/10    Loss: 4.5993952765464785\n",
            "\n",
            "Epoch:    1/10    Loss: 4.551907866954804\n",
            "\n",
            "Epoch:    1/10    Loss: 4.519495310306549\n",
            "\n",
            "Epoch:    1/10    Loss: 4.466621034145355\n",
            "\n",
            "Epoch:    1/10    Loss: 4.431549439907074\n",
            "\n",
            "Epoch:    1/10    Loss: 4.400573610782623\n",
            "\n",
            "Epoch:    1/10    Loss: 4.3581278047561645\n",
            "\n",
            "Epoch:    2/10    Loss: 4.207945308455647\n",
            "\n",
            "Epoch:    2/10    Loss: 4.0819477066993715\n",
            "\n",
            "Epoch:    2/10    Loss: 4.110816679954529\n",
            "\n",
            "Epoch:    2/10    Loss: 4.079642898082733\n",
            "\n",
            "Epoch:    2/10    Loss: 4.054928424358368\n",
            "\n",
            "Epoch:    2/10    Loss: 4.059673748493195\n",
            "\n",
            "Epoch:    2/10    Loss: 4.038173045635223\n",
            "\n",
            "Epoch:    2/10    Loss: 4.032511256694794\n",
            "\n",
            "Epoch:    2/10    Loss: 4.010091172218322\n",
            "\n",
            "Epoch:    2/10    Loss: 3.9886616735458373\n",
            "\n",
            "Epoch:    2/10    Loss: 3.9873407616615295\n",
            "\n",
            "Epoch:    2/10    Loss: 3.993073440074921\n",
            "\n",
            "Epoch:    2/10    Loss: 3.9855278630256654\n",
            "\n",
            "Epoch:    2/10    Loss: 4.002124767780304\n",
            "\n",
            "Epoch:    2/10    Loss: 3.981153263092041\n",
            "\n",
            "Epoch:    2/10    Loss: 3.979147144317627\n",
            "\n",
            "Epoch:    2/10    Loss: 3.9598865151405334\n",
            "\n",
            "Epoch:    2/10    Loss: 3.972624810695648\n",
            "\n",
            "Epoch:    2/10    Loss: 3.9605902662277224\n",
            "\n",
            "Epoch:    2/10    Loss: 3.9211678729057313\n",
            "\n",
            "Epoch:    3/10    Loss: 3.801322784109563\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6934999709129333\n",
            "\n",
            "Epoch:    3/10    Loss: 3.695983113288879\n",
            "\n",
            "Epoch:    3/10    Loss: 3.7057410316467285\n",
            "\n",
            "Epoch:    3/10    Loss: 3.7177302026748658\n",
            "\n",
            "Epoch:    3/10    Loss: 3.7060606679916384\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6848594307899476\n",
            "\n",
            "Epoch:    3/10    Loss: 3.681224593639374\n",
            "\n",
            "Epoch:    3/10    Loss: 3.7052593660354614\n",
            "\n",
            "Epoch:    3/10    Loss: 3.69278545665741\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6771856117248536\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6921730270385744\n",
            "\n",
            "Epoch:    3/10    Loss: 3.705141360759735\n",
            "\n",
            "Epoch:    3/10    Loss: 3.7085375332832338\n",
            "\n",
            "Epoch:    3/10    Loss: 3.7081345434188844\n",
            "\n",
            "Epoch:    3/10    Loss: 3.722042369842529\n",
            "\n",
            "Epoch:    3/10    Loss: 3.715940058708191\n",
            "\n",
            "Epoch:    3/10    Loss: 3.69888943862915\n",
            "\n",
            "Epoch:    3/10    Loss: 3.684733396053314\n",
            "\n",
            "Epoch:    3/10    Loss: 3.6812695474624633\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5763561075933383\n",
            "\n",
            "Epoch:    4/10    Loss: 3.4861314568519592\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5009726481437684\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5075456156730653\n",
            "\n",
            "Epoch:    4/10    Loss: 3.4816886982917787\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5268312702178957\n",
            "\n",
            "Epoch:    4/10    Loss: 3.495212634086609\n",
            "\n",
            "Epoch:    4/10    Loss: 3.506959834575653\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5081105298995974\n",
            "\n",
            "Epoch:    4/10    Loss: 3.542757115840912\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5370429282188414\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5320215554237366\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5265208683013918\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5141133685112\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5239376788139345\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5204371871948243\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5528531284332274\n",
            "\n",
            "Epoch:    4/10    Loss: 3.5518605718612672\n",
            "\n",
            "Epoch:    4/10    Loss: 3.553494607448578\n",
            "\n",
            "Epoch:    4/10    Loss: 3.558679747104645\n",
            "\n",
            "Epoch:    5/10    Loss: 3.420794623584046\n",
            "\n",
            "Epoch:    5/10    Loss: 3.3224429726600646\n",
            "\n",
            "Epoch:    5/10    Loss: 3.3414673218727113\n",
            "\n",
            "Epoch:    5/10    Loss: 3.3644806056022643\n",
            "\n",
            "Epoch:    5/10    Loss: 3.344080048084259\n",
            "\n",
            "Epoch:    5/10    Loss: 3.366133942604065\n",
            "\n",
            "Epoch:    5/10    Loss: 3.385274013996124\n",
            "\n",
            "Epoch:    5/10    Loss: 3.400733650684357\n",
            "\n",
            "Epoch:    5/10    Loss: 3.3696336550712584\n",
            "\n",
            "Epoch:    5/10    Loss: 3.4157583541870116\n",
            "\n",
            "Epoch:    5/10    Loss: 3.4058324909210205\n",
            "\n",
            "Epoch:    5/10    Loss: 3.419599831581116\n",
            "\n",
            "Epoch:    5/10    Loss: 3.4046206941604615\n",
            "\n",
            "Epoch:    5/10    Loss: 3.441532865524292\n",
            "\n",
            "Epoch:    5/10    Loss: 3.4275773220062256\n",
            "\n",
            "Epoch:    5/10    Loss: 3.4269054503440857\n",
            "\n",
            "Epoch:    5/10    Loss: 3.4364962849617005\n",
            "\n",
            "Epoch:    5/10    Loss: 3.4346413412094114\n",
            "\n",
            "Epoch:    5/10    Loss: 3.454922222614288\n",
            "\n",
            "Epoch:    5/10    Loss: 3.466667649269104\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3186324914900824\n",
            "\n",
            "Epoch:    6/10    Loss: 3.245926884651184\n",
            "\n",
            "Epoch:    6/10    Loss: 3.253850317955017\n",
            "\n",
            "Epoch:    6/10    Loss: 3.260084062576294\n",
            "\n",
            "Epoch:    6/10    Loss: 3.2637466168403626\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3011180438995362\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3029092383384704\n",
            "\n",
            "Epoch:    6/10    Loss: 3.262740203380585\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3056028928756716\n",
            "\n",
            "Epoch:    6/10    Loss: 3.309933051586151\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3008756852149963\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3087151312828063\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3128663334846498\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3350264830589293\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3406917653083803\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3362140750885008\n",
            "\n",
            "Epoch:    6/10    Loss: 3.346214136123657\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3621516699790956\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3764428849220276\n",
            "\n",
            "Epoch:    6/10    Loss: 3.3527063579559324\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2264037313570086\n",
            "\n",
            "Epoch:    7/10    Loss: 3.1534934792518614\n",
            "\n",
            "Epoch:    7/10    Loss: 3.1980655732154846\n",
            "\n",
            "Epoch:    7/10    Loss: 3.1803934841156005\n",
            "\n",
            "Epoch:    7/10    Loss: 3.185067665576935\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2084004192352293\n",
            "\n",
            "Epoch:    7/10    Loss: 3.1974707803726194\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2196380338668824\n",
            "\n",
            "Epoch:    7/10    Loss: 3.1998380513191225\n",
            "\n",
            "Epoch:    7/10    Loss: 3.223186068058014\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2457813687324526\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2550720510482787\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2590927090644835\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2603490405082702\n",
            "\n",
            "Epoch:    7/10    Loss: 3.278377341747284\n",
            "\n",
            "Epoch:    7/10    Loss: 3.281583825588226\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2929292783737183\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2987384490966796\n",
            "\n",
            "Epoch:    7/10    Loss: 3.293588744163513\n",
            "\n",
            "Epoch:    7/10    Loss: 3.2989484524726866\n",
            "\n",
            "Epoch:    8/10    Loss: 3.1755756999452154\n",
            "\n",
            "Epoch:    8/10    Loss: 3.085175835609436\n",
            "\n",
            "Epoch:    8/10    Loss: 3.084958571910858\n",
            "\n",
            "Epoch:    8/10    Loss: 3.1333307557106016\n",
            "\n",
            "Epoch:    8/10    Loss: 3.134861171245575\n",
            "\n",
            "Epoch:    8/10    Loss: 3.151192629814148\n",
            "\n",
            "Epoch:    8/10    Loss: 3.1507961111068727\n",
            "\n",
            "Epoch:    8/10    Loss: 3.155526439666748\n",
            "\n",
            "Epoch:    8/10    Loss: 3.1753268575668336\n",
            "\n",
            "Epoch:    8/10    Loss: 3.175088839530945\n",
            "\n",
            "Epoch:    8/10    Loss: 3.203497663021088\n",
            "\n",
            "Epoch:    8/10    Loss: 3.183699910163879\n",
            "\n",
            "Epoch:    8/10    Loss: 3.209415751457214\n",
            "\n",
            "Epoch:    8/10    Loss: 3.203231023311615\n",
            "\n",
            "Epoch:    8/10    Loss: 3.207783715248108\n",
            "\n",
            "Epoch:    8/10    Loss: 3.2217053151130677\n",
            "\n",
            "Epoch:    8/10    Loss: 3.2283244605064394\n",
            "\n",
            "Epoch:    8/10    Loss: 3.2402228288650514\n",
            "\n",
            "Epoch:    8/10    Loss: 3.2279558663368224\n",
            "\n",
            "Epoch:    8/10    Loss: 3.2452004532814027\n",
            "\n",
            "Epoch:    9/10    Loss: 3.13065450424175\n",
            "\n",
            "Epoch:    9/10    Loss: 3.0346641058921815\n",
            "\n",
            "Epoch:    9/10    Loss: 3.0578980932235718\n",
            "\n",
            "Epoch:    9/10    Loss: 3.068038234710693\n",
            "\n",
            "Epoch:    9/10    Loss: 3.0717209901809692\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1107918558120726\n",
            "\n",
            "Epoch:    9/10    Loss: 3.0881654253005983\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1140943145751954\n",
            "\n",
            "Epoch:    9/10    Loss: 3.120715320110321\n",
            "\n",
            "Epoch:    9/10    Loss: 3.125256938457489\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1559981417655947\n",
            "\n",
            "Epoch:    9/10    Loss: 3.147116207122803\n",
            "\n",
            "Epoch:    9/10    Loss: 3.146958960533142\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1659857130050657\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1560533599853517\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1789794597625733\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1593001403808594\n",
            "\n",
            "Epoch:    9/10    Loss: 3.2004432377815246\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1836826272010805\n",
            "\n",
            "Epoch:    9/10    Loss: 3.1891697297096253\n",
            "\n",
            "Epoch:   10/10    Loss: 3.0498503526656195\n",
            "\n",
            "Epoch:   10/10    Loss: 3.02064204454422\n",
            "\n",
            "Epoch:   10/10    Loss: 3.0164082565307617\n",
            "\n",
            "Epoch:   10/10    Loss: 3.0314508395195006\n",
            "\n",
            "Epoch:   10/10    Loss: 3.0507889132499697\n",
            "\n",
            "Epoch:   10/10    Loss: 3.0559671597480773\n",
            "\n",
            "Epoch:   10/10    Loss: 3.035889727592468\n",
            "\n",
            "Epoch:   10/10    Loss: 3.067418406486511\n",
            "\n",
            "Epoch:   10/10    Loss: 3.0666928324699403\n",
            "\n",
            "Epoch:   10/10    Loss: 3.0845796971321104\n",
            "\n",
            "Epoch:   10/10    Loss: 3.09490114402771\n",
            "\n",
            "Epoch:   10/10    Loss: 3.1060717101097106\n",
            "\n",
            "Epoch:   10/10    Loss: 3.1079202709198\n",
            "\n",
            "Epoch:   10/10    Loss: 3.1068125271797182\n",
            "\n",
            "Epoch:   10/10    Loss: 3.134734833240509\n",
            "\n",
            "Epoch:   10/10    Loss: 3.132774360656738\n",
            "\n",
            "Epoch:   10/10    Loss: 3.144491523742676\n",
            "\n",
            "Epoch:   10/10    Loss: 3.129023021221161\n",
            "\n",
            "Epoch:   10/10    Loss: 3.118629505157471\n",
            "\n",
            "Epoch:   10/10    Loss: 3.17928883266449\n",
            "\n",
            "Model Trained and Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "_, vocab_to_int, int_to_vocab, token_dict = load_preprocess()\n",
        "trained_rnn = load_model('./save/trained_rnn')"
      ],
      "metadata": {
        "id": "46X7HnYCEz3N"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
        "    \"\"\"\n",
        "    Generate text using the neural network\n",
        "    :param decoder: The PyTorch Module that holds the trained neural network\n",
        "    :param prime_id: The word id to start the first prediction\n",
        "    :param int_to_vocab: Dict of word id keys to word values\n",
        "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
        "    :param pad_value: The value used to pad a sequence\n",
        "    :param predict_len: The length of text to generate\n",
        "    :return: The generated text\n",
        "    \"\"\"\n",
        "    rnn.eval()\n",
        "\n",
        "    # create a sequence (batch_size=1) with the prime_id\n",
        "    current_seq = np.full((1, sequence_length), pad_value)\n",
        "    current_seq[-1][-1] = prime_id\n",
        "    predicted = [int_to_vocab[prime_id]]\n",
        "\n",
        "    for _ in range(predict_len):\n",
        "        if train_on_gpu:\n",
        "            current_seq = torch.LongTensor(current_seq).cuda()\n",
        "        else:\n",
        "            current_seq = torch.LongTensor(current_seq)\n",
        "\n",
        "        # initialize the hidden state\n",
        "        hidden = rnn.init_hidden(current_seq.size(0))\n",
        "\n",
        "        # get the output of the rnn\n",
        "        output, _ = rnn(current_seq, hidden)\n",
        "\n",
        "        # get the next word probabilities\n",
        "        p = F.softmax(output, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "\n",
        "        # use top_k sampling to get the index of the next word\n",
        "        top_k = 5\n",
        "        p, top_i = p.topk(top_k)\n",
        "        top_i = top_i.numpy().squeeze()\n",
        "\n",
        "        # select the likely next word index with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
        "\n",
        "        # retrieve that word from the dictionary\n",
        "        word = int_to_vocab[word_i]\n",
        "        predicted.append(word)\n",
        "\n",
        "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
        "        current_seq = np.roll(current_seq.cpu(), -1, 1)\n",
        "        current_seq[-1][-1] = word_i\n",
        "\n",
        "    gen_sentences = ' '.join(predicted)\n",
        "\n",
        "    # Replace punctuation tokens\n",
        "    for key, token in token_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
        "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
        "    gen_sentences = gen_sentences.replace('( ', '(')\n",
        "\n",
        "    # return all the sentences\n",
        "    return gen_sentences\n"
      ],
      "metadata": {
        "id": "TYpBmZP_E9HI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_length = 50 # modify the length to your preference\n",
        "prime_words = ['tesla'] # name for starting the script\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "for prime_word in prime_words:\n",
        "    pad_word = SPECIAL_WORDS['PADDING']\n",
        "    generated_script = generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
        "    print(generated_script)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_30uDIKE91_",
        "outputId": "c1c3bce1-7b94-4d01-d7ca-c0014cc56927"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tesla\n",
            "the zacks analyst blog highlights: american airlines, southwest airlines, alaska air group, jetblue airways and southwest airlines\n",
            "alaska air group(alk) in focus: stock falls 5. 2%? tale of the tape\n",
            "stocks to watch: spotlight on walmart, samsung and\n"
          ]
        }
      ]
    }
  ]
}